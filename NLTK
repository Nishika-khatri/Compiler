from nltk.tokenize import word_tokenize, sent_tokenize

text = "Far far away, behind the word mountains. Far from the countries Vokalia and Consonantia, there live the blind texts."

print("Tokenized words")
print(word_tokenize(text))

print("Tokenized sentences")
print(sent_tokenize(text))
